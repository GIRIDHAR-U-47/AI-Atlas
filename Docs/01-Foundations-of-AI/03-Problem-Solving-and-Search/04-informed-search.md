# D:\AI-Atlas\Docs\01-Foundations-of-AI\03-Problem-Solving-and-Search\04-informed-search.md

# Chapter D:\: Informed Search Algorithms

## 1. Introduction to Informed (Heuristic) Search

Informed search, often referred to as heuristic search, utilizes specific domain knowledge to guide the exploration process. Unlike uninformed (blind) search strategies (BFS, DFS, Iterative Deepening), which only distinguish between a goal state and a non-goal state, informed search estimates the promise of a node towards reaching the solution. This guidance dramatically improves efficiency, often at the cost of requiring domain-specific engineering.

### 1.1 The Role of the Heuristic Function

The core component of any informed search strategy is the heuristic function.

**Formal Definition 1.1 (Heuristic Function):**
A heuristic function, denoted $h(n)$, is a function that maps a node $n$ in the search space to a non-negative real number. This value estimates the cost of the cheapest path from the state at node $n$ to a goal state.

$$h(n) = \text{Estimated cost from } n \text{ to goal}$$

If $n$ is the goal state, then $h(n) = 0$.

**Intuitive Analogy:** If you are using a map application (like Google Maps), an uninformed search would be checking every single road in the city blindly. An informed search, guided by a heuristic, uses the "as-the-crow-flies" distance (straight-line distance) to prioritize checking roads that lead generally toward the destination.

**Example: The 8-Puzzle**

Consider the classic 8-puzzle problem (sliding tiles). Common heuristics for a state $n$:

1.  **$h_1(n)$ - Misplaced Tiles:** The count of tiles in the wrong position compared to the goal state.
2.  **$h_2(n)$ - Manhattan Distance:** The sum of the shortest city-block distances (horizontal plus vertical steps) required for each tile to reach its goal position.

Generally, $h_2(n)$ dominates $h_1(n)$ because $h_2(n) \ge h_1(n)$ for all states $n$. A better heuristic (one that is closer to the true cost $h^*(n)$) usually results in fewer nodes expanded.

## 2. Properties of Heuristics

The effectiveness and guarantee of informed search algorithms depend critically on the properties of the heuristic function $h(n)$.

### 2.1 Admissibility

Admissibility is the fundamental requirement for guaranteeing optimality in A* Search.

**Formal Definition 2.1 (Admissible Heuristic):**
A heuristic $h(n)$ is **admissible** if, for every node $n$, the estimated cost $h(n)$ is less than or equal to the actual cost of the optimal path from $n$ to the goal, denoted $h^*(n)$.

$$h(n) \le h^*(n) \quad \text{for all nodes } n$$

Admissible heuristics are **optimistic**; they never overestimate the true cost. If a heuristic is admissible, A* search is guaranteed to find an optimal solution path.

### 2.2 Consistency (Monotonicity)

Consistency is a stricter property than admissibility. It applies to the change in heuristic value between adjacent nodes.

**Formal Definition 2.2 (Consistent Heuristic):**
A heuristic $h(n)$ is **consistent** if, for every node $n$ and every successor $n'$ generated by action $a$ with cost $c(n, a, n')$, the estimated cost $h(n)$ is no greater than the step cost plus the estimated cost from $n'$.

$$h(n) \le c(n, a, n') + h(n') \quad \text{for all nodes } n \text{ and successors } n'$$

**Key Relationship:** Every consistent heuristic is admissible. However, an admissible heuristic is not necessarily consistent. Consistency is particularly useful because it guarantees that the $f$-values along any path are non-decreasing, simplifying the proof for A*'s optimality and efficiency without needing to re-check nodes already expanded (a concept known as graph search efficiency).

---

## 3. Core Informed Search Algorithms

### 3.1 Greedy Best-First Search (GBFS)

Greedy Best-First Search is the simplest form of informed search. It attempts to expand the node that appears closest to the goal.

**Strategy:** GBFS expands the node $n$ with the lowest $h(n)$.

**Characteristics:**
*   **Completeness:** Complete in finite spaces if the state space is a tree (but not if it is a graph and cycles are possible without loop checking).
*   **Optimality:** **Not optimal.** GBFS is purely driven by minimizing the estimated future cost, ignoring the cost incurred so far ($g(n)$). It often finds a quick solution, but not necessarily the shortest one.
*   **Time Complexity:** Generally faster than uninformed searches, but worst case is $O(b^m)$ (where $m$ is the maximum depth), similar to DFS, because it can be fooled by misleading heuristics.

### 3.2 A* Search: The Optimal Informed Search

A* Search is the most widely used informed search algorithm. It combines the advantages of Dijkstra's algorithm (guaranteeing optimality via accumulated path cost $g(n)$) and Greedy Best-First Search (speed via heuristic estimate $h(n)$).

**The Evaluation Function $f(n)$:**
A* evaluates a node $n$ using the function $f(n)$:

$$f(n) = g(n) + h(n)$$

Where:
*   $g(n)$: The cost of the path from the start node to $n$. (Accumulated path cost, guaranteed to be accurate).
*   $h(n)$: The estimated cost of the cheapest path from $n$ to the goal. (Heuristic estimate).
*   $f(n)$: The estimated cost of the cheapest solution path that passes through $n$.

A* always expands the node $n$ in the priority queue (frontier) that has the lowest $f(n)$.

#### 3.2.1 Optimality and Completeness of A*

**Theorem 3.2.1 (A* Optimality):**
If the heuristic function $h(n)$ is **admissible** (never overestimates the true cost), then A* search is guaranteed to find an optimal path to the goal, provided a solution exists.

**Theorem 3.2.2 (A* Optimal Efficiency):**
A* is optimally efficient among all admissible search algorithms. That is, no other admissible algorithm expands fewer nodes than A* (except possibly tie-breaking in $f$-values).

**Space and Time Complexity:**
A*'s complexity is dominated by the size of the search space examined.
*   **Time:** $O(b^d)$ in the worst case, but the actual performance depends heavily on the quality of $h(n)$. If the error in $h(n)$ grows only logarithmically with the path length, A* can approach $O(d)$.
*   **Space:** A* requires storing the frontier (priority queue) and the explored set, leading to $O(b^d)$ space complexity. This high space requirement is its primary drawback.

### 3.3 Managing A*'s Memory Constraint

Due to the memory limits of standard A*, several variants have been developed:

| Algorithm | Description | $f$-value function | Key Advantage |
| :--- | :--- | :--- | :--- |
| **Iterative Deepening A\* (IDA\*)** | Performs a series of Depth-First Searches, restricting the depth based on the $f$-cost limit. | $f(n) = g(n) + h(n)$ | Optimal and complete. Excellent space complexity: $O(d)$. |
| **Recursive Best-First Search (RBFS)** | Uses recursion to implement the search, replacing the standard priority queue with the recursive call stack. Stores the $f$-value of the second-best path in the current branch. | $f(n) = g(n) + h(n)$ | Optimal and complete. Better space efficiency than A*, but often slower due to repeated regeneration of states. |
| **Weighted A\* (WA\*)** | Modifies the $f$-value to prioritize the heuristic, sacrificing optimality for faster search time. | $f(n) = g(n) + W \cdot h(n)$, where $W > 1$ | Significantly faster than A*, but the solution is suboptimal (guaranteed to be within $W$ factor of the optimal cost). |

---

## 4. Misconceptions and Common Pitfalls

### 4.1 Pitfall: Confusing A* and GBFS

The most common mistake is failing to differentiate between the evaluation functions:

*   **A\*:** $f(n) = g(n) + h(n)$ (Path cost + estimated remaining cost). Optimal if $h$ is admissible.
*   **GBFS:** $f(n) = h(n)$ (Estimated remaining cost only). Not optimal.

GBFS is purely "greedy" and may jump to a nearby node only to find that getting there was extremely expensive, leading to a poor solution. A* balances the cost of the path taken so far.

### 4.2 Pitfall: Overestimating Heuristics

Using a heuristic that is **not admissible** ($h(n) > h^*(n)$ for some node $n$) means the algorithm might skip the truly optimal path, believing it to be too expensive. This is because A* prematurely assumes that the true optimal path lies through a node $n'$ where the heuristic underestimates the true cost.

If optimality is required, $h(n)$ must be proven admissible. If speed is the only goal, researchers often use non-admissible heuristics (e.g., in weighted search variants).

### 4.3 Building Effective Heuristics (Relaxed Problems)

Good admissible heuristics are often derived by solving a **relaxed problem**. A relaxed problem is one where some constraints have been removed.

**Definition 4.3.1 (Relaxed Problem Heuristic):**
The cost of an optimal solution to a relaxed version of a problem is an admissible heuristic for the original problem.

**Example (8-Puzzle):**
The Manhattan distance ($h_2$) is derived by relaxing the constraint that a tile can only move into the adjacent empty space. The relaxed problem allows a tile to move directly to its goal position, ignoring other tiles. Since the actual movement must follow stricter rules, the Manhattan distance is always less than or equal to the actual cost, making it admissible.

---

## 5. Connections to Modern AI and LLMs

The principles of informed search, particularly the evaluation function, underpin many advanced AI techniques that rely on search or sequence generation.

### 5.1 Game AI and Deep Learning

In complex game environments (e.g., Chess, Go), search algorithms like Monte Carlo Tree Search (MCTS) or Minimax are fundamentally guided by heuristics.

*   **AlphaGo/AlphaZero:** These systems used deep neural networks (NNs) to generate learned heuristics. The *Value Network* estimates $h(n)$—the probability of winning from a given board state—and the *Policy Network* guides the branching factor. This combines the robust search framework with the generalized pattern recognition power of deep learning.

### 5.2 Planning Systems

Modern automated planning systems (e.g., PDDL solvers) rely heavily on domain-specific or automatically generated heuristics to navigate vast state spaces. Heuristics are often derived from analyzing the preconditions and effects of actions.

### 5.3 Large Language Models (LLMs) and Beam Search

LLMs use a form of guided search for token generation, most commonly **Beam Search**. While not a direct $g(n) + h(n)$ calculation like A*, Beam Search is conceptually informed search:

1.  **Generation:** The model generates probabilities (scores) for the next token.
2.  **Pruning:** It keeps only the top $k$ sequences (the "beam").
3.  **Heuristic Interpretation:** The accumulated log-probability score of a sequence acts as an implicit heuristic, prioritizing sequences that have been statistically more likely so far.

The selection process is informed, aiming for a sequence with a high *cumulative probability*, analogous to prioritizing a path with a low accumulated cost.

## 6. Summary

Informed search leverages domain knowledge, packaged as a heuristic function $h(n)$, to dramatically improve search efficiency compared to blind search methods.

| Algorithm | Expansion Criterion | Guaranteed Optimality | Space Efficiency |
| :--- | :--- | :--- | :--- |
| **GBFS** | $\min(h(n))$ | No | Poor, $O(b^d)$ |
| **A\*** | $\min(g(n) + h(n))$ | Yes (if $h$ is admissible) | Poor, $O(b^d)$ |
| **IDA\*** | $\min(g(n) + h(n))$ | Yes (if $h$ is admissible) | Excellent, $O(d)$ |

The primary theoretical constraints are the properties of $h(n)$: Admissibility ensures optimality, and Consistency strengthens this guarantee and simplifies the search graph maintenance.

---

## 7. Mini Quiz

1.  What is the formal definition of an admissible heuristic $h(n)$? Why is this property critical for A\* Search?
2.  Explain the key difference between the evaluation functions $f(n)$ used in Greedy Best-First Search and A\* Search.
3.  If a pathfinding heuristic is derived from Euclidean distance (straight-line distance), is it likely to be admissible, consistent, both, or neither? Justify your answer.
4.  A researcher uses a **Weighted A\*** algorithm with $W=2$. If the optimal path cost is 10, what is the upper bound guarantee on the cost of the path found by the WA\* algorithm?

---

## 8. Research Bibliography

*   **Artificial Intelligence: A Modern Approach (AIMA), 4th Edition.** Stuart Russell and Peter Norvig. Pearson Education, 2020. (Chapters 3 & 4 provide foundational search concepts and detailed analysis of A*).
*   **"Heuristic Search: Theory and Applications."** Pearl, J. (1984). Addison-Wesley. (A classic text providing the theoretical underpinnings of search algorithms, including optimality proofs).
*   **"Learning Heuristics for A* Search Using Deep Learning."** Arfaee, S. J., Zilles, S., and Holte, R. C. (2010). *Journal of Artificial Intelligence Research (JAIR)*. (Exploration of deriving heuristics automatically rather than manually).
*   **"A Survey of A* Search Variants."** Holte, R. C., & Thayer, I. (2019). *Artificial Intelligence*. (Review of modern memory-efficient and speed-optimized A* variants like IDA* and WA*).